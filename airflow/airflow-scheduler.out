[[34m2024-01-27T20:32:39.111+0000[0m] {[34mscheduler_job_runner.py:[0m788} INFO[0m - Starting the scheduler[0m
[[34m2024-01-27T20:32:39.115+0000[0m] {[34mscheduler_job_runner.py:[0m795} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-01-27T20:32:39.119+0000[0m] {[34mmanager.py:[0m165} INFO[0m - Launched DagFileProcessorManager with pid: 3119[0m
[[34m2024-01-27T20:32:39.121+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T20:32:39.124+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-01-27T20:32:39.151+0000] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-01-27T20:35:28.709+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T20:35:27.499274+00:00 [scheduled]>[0m
[[34m2024-01-27T20:35:28.710+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T20:35:28.710+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T20:35:27.499274+00:00 [scheduled]>[0m
[[34m2024-01-27T20:35:28.712+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T20:35:27.499274+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-01-27T20:35:28.713+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T20:35:27.499274+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:35:28.737+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T20:35:27.499274+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:35:29.871+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T20:35:31.098+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.extract_task manual__2024-01-27T20:35:27.499274+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T20:35:32.790+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T20:35:27.499274+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T20:35:32.797+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=extract_task, run_id=manual__2024-01-27T20:35:27.499274+00:00, map_index=-1, run_start_date=2024-01-27 20:35:31.163928+00:00, run_end_date=2024-01-27 20:35:32.392842+00:00, run_duration=1.228914, state=success, executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-01-27 20:35:28.711273+00:00, queued_by_job_id=15, pid=4325[0m
[[34m2024-01-27T20:35:32.894+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T20:35:27.499274+00:00 [scheduled]>[0m
[[34m2024-01-27T20:35:32.894+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T20:35:32.894+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T20:35:27.499274+00:00 [scheduled]>[0m
[[34m2024-01-27T20:35:32.896+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T20:35:27.499274+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-01-27T20:35:32.896+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T20:35:27.499274+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:35:32.922+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T20:35:27.499274+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:35:33.737+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T20:35:34.816+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.transform_data manual__2024-01-27T20:35:27.499274+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T20:35:35.531+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T20:35:27.499274+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T20:35:35.534+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=transform_data, run_id=manual__2024-01-27T20:35:27.499274+00:00, map_index=-1, run_start_date=2024-01-27 20:35:34.892494+00:00, run_end_date=2024-01-27 20:35:35.141054+00:00, run_duration=0.24856, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-01-27 20:35:32.895125+00:00, queued_by_job_id=15, pid=4347[0m
[[34m2024-01-27T20:35:35.568+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun ETL_1 @ 2024-01-27 20:35:27.499274+00:00: manual__2024-01-27T20:35:27.499274+00:00, state:running, queued_at: 2024-01-27 20:35:27.514529+00:00. externally triggered: True> failed[0m
[[34m2024-01-27T20:35:35.569+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=ETL_1, execution_date=2024-01-27 20:35:27.499274+00:00, run_id=manual__2024-01-27T20:35:27.499274+00:00, run_start_date=2024-01-27 20:35:28.633707+00:00, run_end_date=2024-01-27 20:35:35.569468+00:00, run_duration=6.935761, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-01-27 20:35:27.499274+00:00, data_interval_end=2024-01-27 20:35:27.499274+00:00, dag_hash=37cbb48299113b80a5be6f8b313aaaa2[0m
[[34m2024-01-27T20:37:39.330+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T20:37:47.957+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T20:37:46.581992+00:00 [scheduled]>[0m
[[34m2024-01-27T20:37:47.958+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T20:37:47.959+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T20:37:46.581992+00:00 [scheduled]>[0m
[[34m2024-01-27T20:37:48.071+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T20:37:46.581992+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-01-27T20:37:48.072+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T20:37:46.581992+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:37:48.097+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T20:37:46.581992+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:37:49.198+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T20:37:50.080+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.extract_task manual__2024-01-27T20:37:46.581992+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T20:37:51.577+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T20:37:46.581992+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T20:37:51.582+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=extract_task, run_id=manual__2024-01-27T20:37:46.581992+00:00, map_index=-1, run_start_date=2024-01-27 20:37:50.147627+00:00, run_end_date=2024-01-27 20:37:51.100479+00:00, run_duration=0.952852, state=success, executor_state=success, try_number=1, max_tries=0, job_id=18, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-01-27 20:37:47.959806+00:00, queued_by_job_id=15, pid=5226[0m
[[34m2024-01-27T20:37:51.799+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T20:37:46.581992+00:00 [scheduled]>[0m
[[34m2024-01-27T20:37:51.800+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T20:37:51.800+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T20:37:46.581992+00:00 [scheduled]>[0m
[[34m2024-01-27T20:37:51.801+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T20:37:46.581992+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-01-27T20:37:51.802+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T20:37:46.581992+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:37:51.827+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T20:37:46.581992+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:37:52.710+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T20:37:53.577+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.transform_data manual__2024-01-27T20:37:46.581992+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T20:37:54.331+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T20:37:46.581992+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T20:37:54.334+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=transform_data, run_id=manual__2024-01-27T20:37:46.581992+00:00, map_index=-1, run_start_date=2024-01-27 20:37:53.646572+00:00, run_end_date=2024-01-27 20:37:53.867850+00:00, run_duration=0.221278, state=success, executor_state=success, try_number=1, max_tries=0, job_id=19, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-01-27 20:37:51.800849+00:00, queued_by_job_id=15, pid=5243[0m
[[34m2024-01-27T20:37:54.424+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T20:37:46.581992+00:00 [scheduled]>[0m
[[34m2024-01-27T20:37:54.425+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T20:37:54.425+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T20:37:46.581992+00:00 [scheduled]>[0m
[[34m2024-01-27T20:37:54.462+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T20:37:46.581992+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-01-27T20:37:54.463+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T20:37:46.581992+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:37:54.488+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T20:37:46.581992+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:37:55.304+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T20:37:56.258+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.to_sqlite manual__2024-01-27T20:37:46.581992+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T20:37:57.257+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T20:37:46.581992+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T20:37:57.261+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=to_sqlite, run_id=manual__2024-01-27T20:37:46.581992+00:00, map_index=-1, run_start_date=2024-01-27 20:37:56.326310+00:00, run_end_date=2024-01-27 20:37:56.839981+00:00, run_duration=0.513671, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-01-27 20:37:54.425963+00:00, queued_by_job_id=15, pid=5264[0m
[[34m2024-01-27T20:37:57.329+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun ETL_1 @ 2024-01-27 20:37:46.581992+00:00: manual__2024-01-27T20:37:46.581992+00:00, state:running, queued_at: 2024-01-27 20:37:46.591155+00:00. externally triggered: True> failed[0m
[[34m2024-01-27T20:37:57.329+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=ETL_1, execution_date=2024-01-27 20:37:46.581992+00:00, run_id=manual__2024-01-27T20:37:46.581992+00:00, run_start_date=2024-01-27 20:37:47.682952+00:00, run_end_date=2024-01-27 20:37:57.329357+00:00, run_duration=9.646405, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-01-27 20:37:46.581992+00:00, data_interval_end=2024-01-27 20:37:46.581992+00:00, dag_hash=37cbb48299113b80a5be6f8b313aaaa2[0m
[[34m2024-01-27T20:42:39.358+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T20:47:39.391+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T20:50:53.577+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T20:50:52.794659+00:00 [scheduled]>[0m
[[34m2024-01-27T20:50:53.577+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T20:50:53.578+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T20:50:52.794659+00:00 [scheduled]>[0m
[[34m2024-01-27T20:50:53.579+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T20:50:52.794659+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-01-27T20:50:53.580+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T20:50:52.794659+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:50:53.605+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T20:50:52.794659+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:50:54.527+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T20:50:55.916+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.extract_task manual__2024-01-27T20:50:52.794659+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T20:50:58.020+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T20:50:52.794659+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T20:50:58.023+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=extract_task, run_id=manual__2024-01-27T20:50:52.794659+00:00, map_index=-1, run_start_date=2024-01-27 20:50:56.246947+00:00, run_end_date=2024-01-27 20:50:57.615381+00:00, run_duration=1.368434, state=success, executor_state=success, try_number=1, max_tries=0, job_id=21, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-01-27 20:50:53.578563+00:00, queued_by_job_id=15, pid=9988[0m
[[34m2024-01-27T20:50:58.122+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T20:50:52.794659+00:00 [scheduled]>[0m
[[34m2024-01-27T20:50:58.122+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T20:50:58.122+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T20:50:52.794659+00:00 [scheduled]>[0m
[[34m2024-01-27T20:50:58.124+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T20:50:52.794659+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-01-27T20:50:58.124+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T20:50:52.794659+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:50:58.151+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T20:50:52.794659+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:50:59.015+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T20:50:59.893+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.transform_data manual__2024-01-27T20:50:52.794659+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T20:51:00.558+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T20:50:52.794659+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T20:51:00.562+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=transform_data, run_id=manual__2024-01-27T20:50:52.794659+00:00, map_index=-1, run_start_date=2024-01-27 20:50:59.959064+00:00, run_end_date=2024-01-27 20:51:00.166434+00:00, run_duration=0.20737, state=success, executor_state=success, try_number=1, max_tries=0, job_id=22, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-01-27 20:50:58.123427+00:00, queued_by_job_id=15, pid=10010[0m
[[34m2024-01-27T20:51:00.626+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T20:50:52.794659+00:00 [scheduled]>[0m
[[34m2024-01-27T20:51:00.626+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T20:51:00.626+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T20:50:52.794659+00:00 [scheduled]>[0m
[[34m2024-01-27T20:51:00.628+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T20:50:52.794659+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-01-27T20:51:00.628+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T20:50:52.794659+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:51:00.654+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T20:50:52.794659+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:51:01.441+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T20:51:02.396+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.to_sqlite manual__2024-01-27T20:50:52.794659+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T20:51:03.355+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T20:50:52.794659+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T20:51:03.358+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=to_sqlite, run_id=manual__2024-01-27T20:50:52.794659+00:00, map_index=-1, run_start_date=2024-01-27 20:51:02.463364+00:00, run_end_date=2024-01-27 20:51:02.998255+00:00, run_duration=0.534891, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-01-27 20:51:00.627399+00:00, queued_by_job_id=15, pid=10037[0m
[[34m2024-01-27T20:51:03.422+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun ETL_1 @ 2024-01-27 20:50:52.794659+00:00: manual__2024-01-27T20:50:52.794659+00:00, state:running, queued_at: 2024-01-27 20:50:52.801376+00:00. externally triggered: True> successful[0m
[[34m2024-01-27T20:51:03.422+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=ETL_1, execution_date=2024-01-27 20:50:52.794659+00:00, run_id=manual__2024-01-27T20:50:52.794659+00:00, run_start_date=2024-01-27 20:50:53.512795+00:00, run_end_date=2024-01-27 20:51:03.422736+00:00, run_duration=9.909941, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-27 20:50:52.794659+00:00, data_interval_end=2024-01-27 20:50:52.794659+00:00, dag_hash=30b3308e6bcc13248d04a4e98e72d2ae[0m
[[34m2024-01-27T20:52:39.419+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T20:57:30.934+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T20:57:29.940521+00:00 [scheduled]>[0m
[[34m2024-01-27T20:57:30.935+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T20:57:30.935+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T20:57:29.940521+00:00 [scheduled]>[0m
[[34m2024-01-27T20:57:30.936+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T20:57:29.940521+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-01-27T20:57:30.937+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T20:57:29.940521+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:57:30.963+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T20:57:29.940521+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:57:32.295+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T20:57:33.402+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.extract_task manual__2024-01-27T20:57:29.940521+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T20:57:34.995+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T20:57:29.940521+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T20:57:34.999+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=extract_task, run_id=manual__2024-01-27T20:57:29.940521+00:00, map_index=-1, run_start_date=2024-01-27 20:57:33.488098+00:00, run_end_date=2024-01-27 20:57:34.471358+00:00, run_duration=0.98326, state=success, executor_state=success, try_number=1, max_tries=0, job_id=24, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-01-27 20:57:30.935834+00:00, queued_by_job_id=15, pid=12642[0m
[[34m2024-01-27T20:57:35.102+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T20:57:29.940521+00:00 [scheduled]>[0m
[[34m2024-01-27T20:57:35.102+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T20:57:35.102+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T20:57:29.940521+00:00 [scheduled]>[0m
[[34m2024-01-27T20:57:35.104+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T20:57:29.940521+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-01-27T20:57:35.104+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T20:57:29.940521+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:57:35.129+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T20:57:29.940521+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:57:36.053+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T20:57:36.943+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.transform_data manual__2024-01-27T20:57:29.940521+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T20:57:37.729+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T20:57:29.940521+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T20:57:37.732+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=transform_data, run_id=manual__2024-01-27T20:57:29.940521+00:00, map_index=-1, run_start_date=2024-01-27 20:57:37.014963+00:00, run_end_date=2024-01-27 20:57:37.230976+00:00, run_duration=0.216013, state=success, executor_state=success, try_number=1, max_tries=0, job_id=25, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-01-27 20:57:35.103326+00:00, queued_by_job_id=15, pid=12658[0m
[[34m2024-01-27T20:57:37.802+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T20:57:29.940521+00:00 [scheduled]>[0m
[[34m2024-01-27T20:57:37.803+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T20:57:37.803+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T20:57:29.940521+00:00 [scheduled]>[0m
[[34m2024-01-27T20:57:37.804+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T20:57:29.940521+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-01-27T20:57:37.805+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T20:57:29.940521+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:57:37.833+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T20:57:29.940521+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T20:57:38.781+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T20:57:39.894+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.to_sqlite manual__2024-01-27T20:57:29.940521+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T20:57:40.722+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T20:57:29.940521+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T20:57:40.725+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=to_sqlite, run_id=manual__2024-01-27T20:57:29.940521+00:00, map_index=-1, run_start_date=2024-01-27 20:57:39.968590+00:00, run_end_date=2024-01-27 20:57:40.254920+00:00, run_duration=0.28633, state=success, executor_state=success, try_number=1, max_tries=0, job_id=26, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-01-27 20:57:37.803776+00:00, queued_by_job_id=15, pid=12691[0m
[[34m2024-01-27T20:57:40.765+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T20:57:40.798+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun ETL_1 @ 2024-01-27 20:57:29.940521+00:00: manual__2024-01-27T20:57:29.940521+00:00, state:running, queued_at: 2024-01-27 20:57:29.946867+00:00. externally triggered: True> successful[0m
[[34m2024-01-27T20:57:40.800+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=ETL_1, execution_date=2024-01-27 20:57:29.940521+00:00, run_id=manual__2024-01-27T20:57:29.940521+00:00, run_start_date=2024-01-27 20:57:30.860731+00:00, run_end_date=2024-01-27 20:57:40.800523+00:00, run_duration=9.939792, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-27 20:57:29.940521+00:00, data_interval_end=2024-01-27 20:57:29.940521+00:00, dag_hash=30b3308e6bcc13248d04a4e98e72d2ae[0m
[[34m2024-01-27T21:02:40.780+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T21:04:26.499+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T21:04:25.098989+00:00 [scheduled]>[0m
[[34m2024-01-27T21:04:26.499+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T21:04:26.499+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T21:04:25.098989+00:00 [scheduled]>[0m
[[34m2024-01-27T21:04:26.501+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T21:04:25.098989+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-01-27T21:04:26.501+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T21:04:25.098989+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T21:04:26.526+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T21:04:25.098989+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T21:04:28.117+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T21:04:29.303+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.extract_task manual__2024-01-27T21:04:25.098989+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T21:04:30.877+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T21:04:25.098989+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T21:04:30.881+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=extract_task, run_id=manual__2024-01-27T21:04:25.098989+00:00, map_index=-1, run_start_date=2024-01-27 21:04:29.374082+00:00, run_end_date=2024-01-27 21:04:30.431097+00:00, run_duration=1.057015, state=success, executor_state=success, try_number=1, max_tries=0, job_id=27, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-01-27 21:04:26.500119+00:00, queued_by_job_id=15, pid=15336[0m
[[34m2024-01-27T21:04:30.983+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T21:04:25.098989+00:00 [scheduled]>[0m
[[34m2024-01-27T21:04:30.983+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T21:04:30.983+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T21:04:25.098989+00:00 [scheduled]>[0m
[[34m2024-01-27T21:04:30.985+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T21:04:25.098989+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-01-27T21:04:30.985+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T21:04:25.098989+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T21:04:31.012+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T21:04:25.098989+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T21:04:31.897+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T21:04:32.808+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.transform_data manual__2024-01-27T21:04:25.098989+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T21:04:33.707+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T21:04:25.098989+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T21:04:33.714+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=transform_data, run_id=manual__2024-01-27T21:04:25.098989+00:00, map_index=-1, run_start_date=2024-01-27 21:04:32.893035+00:00, run_end_date=2024-01-27 21:04:33.123324+00:00, run_duration=0.230289, state=success, executor_state=success, try_number=1, max_tries=0, job_id=28, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-01-27 21:04:30.984096+00:00, queued_by_job_id=15, pid=15349[0m
[[34m2024-01-27T21:04:33.792+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T21:04:25.098989+00:00 [scheduled]>[0m
[[34m2024-01-27T21:04:33.792+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T21:04:33.792+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T21:04:25.098989+00:00 [scheduled]>[0m
[[34m2024-01-27T21:04:33.794+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T21:04:25.098989+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-01-27T21:04:33.794+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T21:04:25.098989+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T21:04:33.821+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T21:04:25.098989+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T21:04:34.636+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T21:04:35.663+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.to_sqlite manual__2024-01-27T21:04:25.098989+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T21:04:36.531+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T21:04:25.098989+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T21:04:36.534+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=to_sqlite, run_id=manual__2024-01-27T21:04:25.098989+00:00, map_index=-1, run_start_date=2024-01-27 21:04:35.779394+00:00, run_end_date=2024-01-27 21:04:36.094069+00:00, run_duration=0.314675, state=success, executor_state=success, try_number=1, max_tries=0, job_id=29, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-01-27 21:04:33.793169+00:00, queued_by_job_id=15, pid=15388[0m
[[34m2024-01-27T21:04:36.605+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun ETL_1 @ 2024-01-27 21:04:25.098989+00:00: manual__2024-01-27T21:04:25.098989+00:00, state:running, queued_at: 2024-01-27 21:04:25.105829+00:00. externally triggered: True> successful[0m
[[34m2024-01-27T21:04:36.605+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=ETL_1, execution_date=2024-01-27 21:04:25.098989+00:00, run_id=manual__2024-01-27T21:04:25.098989+00:00, run_start_date=2024-01-27 21:04:26.434257+00:00, run_end_date=2024-01-27 21:04:36.605492+00:00, run_duration=10.171235, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-27 21:04:25.098989+00:00, data_interval_end=2024-01-27 21:04:25.098989+00:00, dag_hash=1e27ab939cc1f9bb044d3cc092fb9b1b[0m
[[34m2024-01-27T21:07:40.809+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T21:12:40.847+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T21:17:40.920+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T21:22:40.925+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T21:27:41.056+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T21:32:41.083+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T21:37:41.112+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T21:42:41.150+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T21:47:41.178+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T21:52:41.412+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T21:57:41.418+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T22:02:41.446+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T22:06:56.073+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:06:54.591792+00:00 [scheduled]>[0m
[[34m2024-01-27T22:06:56.073+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:06:56.073+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:06:54.591792+00:00 [scheduled]>[0m
[[34m2024-01-27T22:06:56.075+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:06:54.591792+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-01-27T22:06:56.075+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:06:54.591792+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:06:56.099+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:06:54.591792+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:06:57.378+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:06:58.389+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.extract_task manual__2024-01-27T22:06:54.591792+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:07:00.077+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:06:54.591792+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:07:00.081+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=extract_task, run_id=manual__2024-01-27T22:06:54.591792+00:00, map_index=-1, run_start_date=2024-01-27 22:06:58.458282+00:00, run_end_date=2024-01-27 22:06:59.653515+00:00, run_duration=1.195233, state=success, executor_state=success, try_number=1, max_tries=0, job_id=30, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-01-27 22:06:56.074230+00:00, queued_by_job_id=15, pid=38746[0m
[[34m2024-01-27T22:07:00.303+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:06:54.591792+00:00 [scheduled]>[0m
[[34m2024-01-27T22:07:00.304+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:07:00.304+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:06:54.591792+00:00 [scheduled]>[0m
[[34m2024-01-27T22:07:00.305+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:06:54.591792+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-01-27T22:07:00.305+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:06:54.591792+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:07:00.330+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:06:54.591792+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:07:01.167+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:07:02.054+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.transform_data manual__2024-01-27T22:06:54.591792+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:07:02.799+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:06:54.591792+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:07:02.802+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=transform_data, run_id=manual__2024-01-27T22:06:54.591792+00:00, map_index=-1, run_start_date=2024-01-27 22:07:02.130454+00:00, run_end_date=2024-01-27 22:07:02.353016+00:00, run_duration=0.222562, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=31, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-01-27 22:07:00.304701+00:00, queued_by_job_id=15, pid=38788[0m
[[34m2024-01-27T22:07:03.903+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun ETL_1 @ 2024-01-27 22:06:54.591792+00:00: manual__2024-01-27T22:06:54.591792+00:00, state:running, queued_at: 2024-01-27 22:06:54.598288+00:00. externally triggered: True> failed[0m
[[34m2024-01-27T22:07:03.903+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=ETL_1, execution_date=2024-01-27 22:06:54.591792+00:00, run_id=manual__2024-01-27T22:06:54.591792+00:00, run_start_date=2024-01-27 22:06:56.004122+00:00, run_end_date=2024-01-27 22:07:03.903439+00:00, run_duration=7.899317, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-01-27 22:06:54.591792+00:00, data_interval_end=2024-01-27 22:06:54.591792+00:00, dag_hash=5f309e1ddb99bb2784a3ea38ecdeb376[0m
[[34m2024-01-27T22:07:41.483+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T22:09:04.373+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:09:03.595700+00:00 [scheduled]>[0m
[[34m2024-01-27T22:09:04.373+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:09:04.373+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:09:03.595700+00:00 [scheduled]>[0m
[[34m2024-01-27T22:09:04.375+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:09:03.595700+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-01-27T22:09:04.375+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:09:03.595700+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:09:04.400+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:09:03.595700+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:09:05.463+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:09:06.539+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.extract_task manual__2024-01-27T22:09:03.595700+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:09:08.026+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:09:03.595700+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:09:08.029+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=extract_task, run_id=manual__2024-01-27T22:09:03.595700+00:00, map_index=-1, run_start_date=2024-01-27 22:09:06.607245+00:00, run_end_date=2024-01-27 22:09:07.582363+00:00, run_duration=0.975118, state=success, executor_state=success, try_number=1, max_tries=0, job_id=32, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-01-27 22:09:04.374255+00:00, queued_by_job_id=15, pid=39585[0m
[[34m2024-01-27T22:09:08.126+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:09:03.595700+00:00 [scheduled]>[0m
[[34m2024-01-27T22:09:08.127+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:09:08.127+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:09:03.595700+00:00 [scheduled]>[0m
[[34m2024-01-27T22:09:08.128+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:09:03.595700+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-01-27T22:09:08.129+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:09:03.595700+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:09:08.153+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:09:03.595700+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:09:09.004+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:09:09.946+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.transform_data manual__2024-01-27T22:09:03.595700+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:09:10.776+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:09:03.595700+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:09:10.779+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=transform_data, run_id=manual__2024-01-27T22:09:03.595700+00:00, map_index=-1, run_start_date=2024-01-27 22:09:10.023090+00:00, run_end_date=2024-01-27 22:09:10.268124+00:00, run_duration=0.245034, state=success, executor_state=success, try_number=1, max_tries=0, job_id=33, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-01-27 22:09:08.127841+00:00, queued_by_job_id=15, pid=39601[0m
[[34m2024-01-27T22:09:10.848+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:09:03.595700+00:00 [scheduled]>[0m
[[34m2024-01-27T22:09:10.848+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:09:10.848+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:09:03.595700+00:00 [scheduled]>[0m
[[34m2024-01-27T22:09:10.850+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='construct_load_command', run_id='manual__2024-01-27T22:09:03.595700+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-01-27T22:09:10.850+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'construct_load_command', 'manual__2024-01-27T22:09:03.595700+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:09:10.877+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'construct_load_command', 'manual__2024-01-27T22:09:03.595700+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:09:11.892+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:09:12.958+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:09:03.595700+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:09:13.755+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='construct_load_command', run_id='manual__2024-01-27T22:09:03.595700+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:09:13.759+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=construct_load_command, run_id=manual__2024-01-27T22:09:03.595700+00:00, map_index=-1, run_start_date=2024-01-27 22:09:13.030774+00:00, run_end_date=2024-01-27 22:09:13.255470+00:00, run_duration=0.224696, state=success, executor_state=success, try_number=1, max_tries=0, job_id=34, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-01-27 22:09:10.849041+00:00, queued_by_job_id=15, pid=39633[0m
[[34m2024-01-27T22:09:13.865+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:09:03.595700+00:00 [scheduled]>[0m
[[34m2024-01-27T22:09:13.865+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:09:13.865+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:09:03.595700+00:00 [scheduled]>[0m
[[34m2024-01-27T22:09:13.867+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T22:09:03.595700+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-01-27T22:09:13.867+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T22:09:03.595700+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:09:13.892+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T22:09:03.595700+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:09:14.742+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:09:15.627+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:09:03.595700+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:09:16.545+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T22:09:03.595700+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:09:16.548+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=to_sqlite, run_id=manual__2024-01-27T22:09:03.595700+00:00, map_index=-1, run_start_date=2024-01-27 22:09:15.705167+00:00, run_end_date=2024-01-27 22:09:16.010083+00:00, run_duration=0.304916, state=success, executor_state=success, try_number=1, max_tries=0, job_id=35, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-01-27 22:09:13.866364+00:00, queued_by_job_id=15, pid=39642[0m
[[34m2024-01-27T22:09:16.596+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun ETL_1 @ 2024-01-27 22:09:03.595700+00:00: manual__2024-01-27T22:09:03.595700+00:00, state:running, queued_at: 2024-01-27 22:09:03.601198+00:00. externally triggered: True> successful[0m
[[34m2024-01-27T22:09:16.596+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=ETL_1, execution_date=2024-01-27 22:09:03.595700+00:00, run_id=manual__2024-01-27T22:09:03.595700+00:00, run_start_date=2024-01-27 22:09:04.256304+00:00, run_end_date=2024-01-27 22:09:16.596854+00:00, run_duration=12.34055, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-27 22:09:03.595700+00:00, data_interval_end=2024-01-27 22:09:03.595700+00:00, dag_hash=5f309e1ddb99bb2784a3ea38ecdeb376[0m
[[34m2024-01-27T22:12:41.512+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T22:13:44.777+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:13:43.373076+00:00 [scheduled]>[0m
[[34m2024-01-27T22:13:44.777+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:13:44.777+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:13:43.373076+00:00 [scheduled]>[0m
[[34m2024-01-27T22:13:44.779+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:13:43.373076+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-01-27T22:13:44.779+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:13:43.373076+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:13:44.806+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:13:43.373076+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:13:46.231+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:13:47.347+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.extract_task manual__2024-01-27T22:13:43.373076+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:13:48.831+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:13:43.373076+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:13:48.834+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=extract_task, run_id=manual__2024-01-27T22:13:43.373076+00:00, map_index=-1, run_start_date=2024-01-27 22:13:47.416244+00:00, run_end_date=2024-01-27 22:13:48.365234+00:00, run_duration=0.94899, state=success, executor_state=success, try_number=1, max_tries=0, job_id=36, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-01-27 22:13:44.778069+00:00, queued_by_job_id=15, pid=41626[0m
[[34m2024-01-27T22:13:48.933+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:13:43.373076+00:00 [scheduled]>[0m
[[34m2024-01-27T22:13:48.933+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:13:48.933+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:13:43.373076+00:00 [scheduled]>[0m
[[34m2024-01-27T22:13:48.935+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:13:43.373076+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-01-27T22:13:48.935+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:13:43.373076+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:13:48.960+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:13:43.373076+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:13:49.796+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:13:50.649+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.transform_data manual__2024-01-27T22:13:43.373076+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:13:51.375+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:13:43.373076+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:13:51.379+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=transform_data, run_id=manual__2024-01-27T22:13:43.373076+00:00, map_index=-1, run_start_date=2024-01-27 22:13:50.720440+00:00, run_end_date=2024-01-27 22:13:50.971080+00:00, run_duration=0.25064, state=success, executor_state=success, try_number=1, max_tries=0, job_id=37, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-01-27 22:13:48.934171+00:00, queued_by_job_id=15, pid=41648[0m
[[34m2024-01-27T22:13:51.447+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:13:43.373076+00:00 [scheduled]>[0m
[[34m2024-01-27T22:13:51.447+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:13:51.448+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:13:43.373076+00:00 [scheduled]>[0m
[[34m2024-01-27T22:13:51.449+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='construct_load_command', run_id='manual__2024-01-27T22:13:43.373076+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-01-27T22:13:51.449+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'construct_load_command', 'manual__2024-01-27T22:13:43.373076+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:13:51.475+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'construct_load_command', 'manual__2024-01-27T22:13:43.373076+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:13:52.373+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:13:53.277+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:13:43.373076+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:13:54.042+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='construct_load_command', run_id='manual__2024-01-27T22:13:43.373076+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:13:54.046+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=construct_load_command, run_id=manual__2024-01-27T22:13:43.373076+00:00, map_index=-1, run_start_date=2024-01-27 22:13:53.350124+00:00, run_end_date=2024-01-27 22:13:53.567006+00:00, run_duration=0.216882, state=success, executor_state=success, try_number=1, max_tries=0, job_id=38, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-01-27 22:13:51.448567+00:00, queued_by_job_id=15, pid=41657[0m
[[34m2024-01-27T22:13:54.141+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:13:43.373076+00:00 [scheduled]>[0m
[[34m2024-01-27T22:13:54.141+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:13:54.141+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:13:43.373076+00:00 [scheduled]>[0m
[[34m2024-01-27T22:13:54.143+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T22:13:43.373076+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-01-27T22:13:54.143+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T22:13:43.373076+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:13:54.167+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T22:13:43.373076+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:13:55.026+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:13:55.856+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:13:43.373076+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:13:56.675+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T22:13:43.373076+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:13:56.680+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=to_sqlite, run_id=manual__2024-01-27T22:13:43.373076+00:00, map_index=-1, run_start_date=2024-01-27 22:13:55.930965+00:00, run_end_date=2024-01-27 22:13:56.231837+00:00, run_duration=0.300872, state=success, executor_state=success, try_number=1, max_tries=0, job_id=39, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-01-27 22:13:54.142266+00:00, queued_by_job_id=15, pid=41672[0m
[[34m2024-01-27T22:13:56.727+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun ETL_1 @ 2024-01-27 22:13:43.373076+00:00: manual__2024-01-27T22:13:43.373076+00:00, state:running, queued_at: 2024-01-27 22:13:43.381256+00:00. externally triggered: True> successful[0m
[[34m2024-01-27T22:13:56.728+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=ETL_1, execution_date=2024-01-27 22:13:43.373076+00:00, run_id=manual__2024-01-27T22:13:43.373076+00:00, run_start_date=2024-01-27 22:13:44.707740+00:00, run_end_date=2024-01-27 22:13:56.728440+00:00, run_duration=12.0207, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-27 22:13:43.373076+00:00, data_interval_end=2024-01-27 22:13:43.373076+00:00, dag_hash=5f309e1ddb99bb2784a3ea38ecdeb376[0m
[[34m2024-01-27T22:17:41.541+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T22:22:41.545+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T22:25:27.307+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:25:26.036149+00:00 [scheduled]>[0m
[[34m2024-01-27T22:25:27.307+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:25:27.307+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:25:26.036149+00:00 [scheduled]>[0m
[[34m2024-01-27T22:25:27.309+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:25:26.036149+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-01-27T22:25:27.309+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:25:26.036149+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:25:27.335+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:25:26.036149+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:25:28.305+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:25:29.256+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.extract_task manual__2024-01-27T22:25:26.036149+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:25:30.714+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:25:26.036149+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:25:30.718+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=extract_task, run_id=manual__2024-01-27T22:25:26.036149+00:00, map_index=-1, run_start_date=2024-01-27 22:25:29.325472+00:00, run_end_date=2024-01-27 22:25:30.281271+00:00, run_duration=0.955799, state=success, executor_state=success, try_number=1, max_tries=0, job_id=40, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-01-27 22:25:27.308062+00:00, queued_by_job_id=15, pid=46091[0m
[[34m2024-01-27T22:25:30.818+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:25:26.036149+00:00 [scheduled]>[0m
[[34m2024-01-27T22:25:30.818+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:25:30.819+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:25:26.036149+00:00 [scheduled]>[0m
[[34m2024-01-27T22:25:30.820+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:25:26.036149+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-01-27T22:25:30.821+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:25:26.036149+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:25:30.845+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:25:26.036149+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:25:31.635+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:25:32.588+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.transform_data manual__2024-01-27T22:25:26.036149+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:25:33.307+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:25:26.036149+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:25:33.311+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=transform_data, run_id=manual__2024-01-27T22:25:26.036149+00:00, map_index=-1, run_start_date=2024-01-27 22:25:32.658913+00:00, run_end_date=2024-01-27 22:25:32.901429+00:00, run_duration=0.242516, state=success, executor_state=success, try_number=1, max_tries=0, job_id=41, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-01-27 22:25:30.819477+00:00, queued_by_job_id=15, pid=46107[0m
[[34m2024-01-27T22:25:33.380+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:25:26.036149+00:00 [scheduled]>[0m
[[34m2024-01-27T22:25:33.380+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:25:33.380+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:25:26.036149+00:00 [scheduled]>[0m
[[34m2024-01-27T22:25:33.382+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='construct_load_command', run_id='manual__2024-01-27T22:25:26.036149+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-01-27T22:25:33.382+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'construct_load_command', 'manual__2024-01-27T22:25:26.036149+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:25:33.409+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'construct_load_command', 'manual__2024-01-27T22:25:26.036149+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:25:34.426+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:25:35.276+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:25:26.036149+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:25:35.966+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='construct_load_command', run_id='manual__2024-01-27T22:25:26.036149+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:25:35.969+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=construct_load_command, run_id=manual__2024-01-27T22:25:26.036149+00:00, map_index=-1, run_start_date=2024-01-27 22:25:35.344672+00:00, run_end_date=2024-01-27 22:25:35.562908+00:00, run_duration=0.218236, state=success, executor_state=success, try_number=1, max_tries=0, job_id=42, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-01-27 22:25:33.381127+00:00, queued_by_job_id=15, pid=46122[0m
[[34m2024-01-27T22:25:36.088+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:25:26.036149+00:00 [scheduled]>[0m
[[34m2024-01-27T22:25:36.088+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:25:36.089+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:25:26.036149+00:00 [scheduled]>[0m
[[34m2024-01-27T22:25:36.090+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T22:25:26.036149+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-01-27T22:25:36.091+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T22:25:26.036149+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:25:36.116+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T22:25:26.036149+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:25:36.920+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:25:37.871+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:25:26.036149+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:25:38.603+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T22:25:26.036149+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:25:38.606+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=to_sqlite, run_id=manual__2024-01-27T22:25:26.036149+00:00, map_index=-1, run_start_date=2024-01-27 22:25:37.945305+00:00, run_end_date=2024-01-27 22:25:38.232191+00:00, run_duration=0.286886, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=43, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-01-27 22:25:36.089534+00:00, queued_by_job_id=15, pid=46131[0m
[[34m2024-01-27T22:25:38.640+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun ETL_1 @ 2024-01-27 22:25:26.036149+00:00: manual__2024-01-27T22:25:26.036149+00:00, state:running, queued_at: 2024-01-27 22:25:26.042819+00:00. externally triggered: True> failed[0m
[[34m2024-01-27T22:25:38.640+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=ETL_1, execution_date=2024-01-27 22:25:26.036149+00:00, run_id=manual__2024-01-27T22:25:26.036149+00:00, run_start_date=2024-01-27 22:25:27.205424+00:00, run_end_date=2024-01-27 22:25:38.640667+00:00, run_duration=11.435243, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-01-27 22:25:26.036149+00:00, data_interval_end=2024-01-27 22:25:26.036149+00:00, dag_hash=c05957d759d59c662d3ec685c0a4445b[0m
[[34m2024-01-27T22:27:41.573+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T22:28:01.924+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:28:00.690513+00:00 [scheduled]>[0m
[[34m2024-01-27T22:28:01.924+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:28:01.924+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:28:00.690513+00:00 [scheduled]>[0m
[[34m2024-01-27T22:28:01.926+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:28:00.690513+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-01-27T22:28:01.926+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:28:00.690513+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:28:01.951+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:28:00.690513+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:28:02.870+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:28:03.748+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.extract_task manual__2024-01-27T22:28:00.690513+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:28:05.173+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:28:00.690513+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:28:05.177+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=extract_task, run_id=manual__2024-01-27T22:28:00.690513+00:00, map_index=-1, run_start_date=2024-01-27 22:28:03.821895+00:00, run_end_date=2024-01-27 22:28:04.755762+00:00, run_duration=0.933867, state=success, executor_state=success, try_number=1, max_tries=0, job_id=44, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-01-27 22:28:01.925047+00:00, queued_by_job_id=15, pid=47110[0m
[[34m2024-01-27T22:28:05.277+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:28:00.690513+00:00 [scheduled]>[0m
[[34m2024-01-27T22:28:05.278+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:28:05.278+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:28:00.690513+00:00 [scheduled]>[0m
[[34m2024-01-27T22:28:05.279+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:28:00.690513+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-01-27T22:28:05.280+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:28:00.690513+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:28:05.304+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:28:00.690513+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:28:06.150+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:28:07.052+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.transform_data manual__2024-01-27T22:28:00.690513+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:28:07.807+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:28:00.690513+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:28:07.810+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=transform_data, run_id=manual__2024-01-27T22:28:00.690513+00:00, map_index=-1, run_start_date=2024-01-27 22:28:07.125682+00:00, run_end_date=2024-01-27 22:28:07.376627+00:00, run_duration=0.250945, state=success, executor_state=success, try_number=1, max_tries=0, job_id=45, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-01-27 22:28:05.278869+00:00, queued_by_job_id=15, pid=47126[0m
[[34m2024-01-27T22:28:07.886+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:28:00.690513+00:00 [scheduled]>[0m
[[34m2024-01-27T22:28:07.886+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:28:07.886+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:28:00.690513+00:00 [scheduled]>[0m
[[34m2024-01-27T22:28:07.888+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='construct_load_command', run_id='manual__2024-01-27T22:28:00.690513+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-01-27T22:28:07.888+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'construct_load_command', 'manual__2024-01-27T22:28:00.690513+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:28:07.913+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'construct_load_command', 'manual__2024-01-27T22:28:00.690513+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:28:08.770+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:28:09.668+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:28:00.690513+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:28:10.505+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='construct_load_command', run_id='manual__2024-01-27T22:28:00.690513+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:28:10.509+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=construct_load_command, run_id=manual__2024-01-27T22:28:00.690513+00:00, map_index=-1, run_start_date=2024-01-27 22:28:09.759642+00:00, run_end_date=2024-01-27 22:28:10.008214+00:00, run_duration=0.248572, state=success, executor_state=success, try_number=1, max_tries=0, job_id=46, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-01-27 22:28:07.887044+00:00, queued_by_job_id=15, pid=47141[0m
[[34m2024-01-27T22:28:10.607+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:28:00.690513+00:00 [scheduled]>[0m
[[34m2024-01-27T22:28:10.608+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:28:10.608+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:28:00.690513+00:00 [scheduled]>[0m
[[34m2024-01-27T22:28:10.609+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T22:28:00.690513+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-01-27T22:28:10.610+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T22:28:00.690513+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:28:10.635+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T22:28:00.690513+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:28:11.432+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:28:12.405+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:28:00.690513+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:28:13.135+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T22:28:00.690513+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:28:13.139+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=to_sqlite, run_id=manual__2024-01-27T22:28:00.690513+00:00, map_index=-1, run_start_date=2024-01-27 22:28:12.474913+00:00, run_end_date=2024-01-27 22:28:12.733256+00:00, run_duration=0.258343, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=47, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-01-27 22:28:10.608855+00:00, queued_by_job_id=15, pid=47174[0m
[[34m2024-01-27T22:28:13.173+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun ETL_1 @ 2024-01-27 22:28:00.690513+00:00: manual__2024-01-27T22:28:00.690513+00:00, state:running, queued_at: 2024-01-27 22:28:00.695758+00:00. externally triggered: True> failed[0m
[[34m2024-01-27T22:28:13.173+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=ETL_1, execution_date=2024-01-27 22:28:00.690513+00:00, run_id=manual__2024-01-27T22:28:00.690513+00:00, run_start_date=2024-01-27 22:28:01.767400+00:00, run_end_date=2024-01-27 22:28:13.173567+00:00, run_duration=11.406167, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-01-27 22:28:00.690513+00:00, data_interval_end=2024-01-27 22:28:00.690513+00:00, dag_hash=c05957d759d59c662d3ec685c0a4445b[0m
[[34m2024-01-27T22:32:40.517+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:32:39.607169+00:00 [scheduled]>[0m
[[34m2024-01-27T22:32:40.517+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:32:40.517+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:32:39.607169+00:00 [scheduled]>[0m
[[34m2024-01-27T22:32:40.519+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:32:39.607169+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-01-27T22:32:40.519+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:32:39.607169+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:32:40.545+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:32:39.607169+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:32:41.545+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:32:42.494+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.extract_task manual__2024-01-27T22:32:39.607169+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:32:44.474+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:32:39.607169+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:32:44.477+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=extract_task, run_id=manual__2024-01-27T22:32:39.607169+00:00, map_index=-1, run_start_date=2024-01-27 22:32:42.560058+00:00, run_end_date=2024-01-27 22:32:44.065450+00:00, run_duration=1.505392, state=success, executor_state=success, try_number=1, max_tries=0, job_id=48, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-01-27 22:32:40.518057+00:00, queued_by_job_id=15, pid=48899[0m
[[34m2024-01-27T22:32:44.518+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T22:32:44.577+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:32:39.607169+00:00 [scheduled]>[0m
[[34m2024-01-27T22:32:44.577+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:32:44.578+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:32:39.607169+00:00 [scheduled]>[0m
[[34m2024-01-27T22:32:44.579+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:32:39.607169+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-01-27T22:32:44.579+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:32:39.607169+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:32:44.605+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:32:39.607169+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:32:45.388+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:32:46.265+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.transform_data manual__2024-01-27T22:32:39.607169+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:32:47.131+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:32:39.607169+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:32:47.135+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=transform_data, run_id=manual__2024-01-27T22:32:39.607169+00:00, map_index=-1, run_start_date=2024-01-27 22:32:46.338015+00:00, run_end_date=2024-01-27 22:32:46.576682+00:00, run_duration=0.238667, state=success, executor_state=success, try_number=1, max_tries=0, job_id=49, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-01-27 22:32:44.578654+00:00, queued_by_job_id=15, pid=48915[0m
[[34m2024-01-27T22:32:47.201+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:32:39.607169+00:00 [scheduled]>[0m
[[34m2024-01-27T22:32:47.202+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:32:47.202+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:32:39.607169+00:00 [scheduled]>[0m
[[34m2024-01-27T22:32:47.203+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='construct_load_command', run_id='manual__2024-01-27T22:32:39.607169+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-01-27T22:32:47.203+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'construct_load_command', 'manual__2024-01-27T22:32:39.607169+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:32:47.229+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'construct_load_command', 'manual__2024-01-27T22:32:39.607169+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:32:48.165+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:32:49.051+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:32:39.607169+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:32:49.744+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='construct_load_command', run_id='manual__2024-01-27T22:32:39.607169+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:32:49.748+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=construct_load_command, run_id=manual__2024-01-27T22:32:39.607169+00:00, map_index=-1, run_start_date=2024-01-27 22:32:49.126202+00:00, run_end_date=2024-01-27 22:32:49.350451+00:00, run_duration=0.224249, state=success, executor_state=success, try_number=1, max_tries=0, job_id=50, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-01-27 22:32:47.202765+00:00, queued_by_job_id=15, pid=48947[0m
[[34m2024-01-27T22:32:49.840+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:32:39.607169+00:00 [scheduled]>[0m
[[34m2024-01-27T22:32:49.840+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:32:49.840+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:32:39.607169+00:00 [scheduled]>[0m
[[34m2024-01-27T22:32:49.842+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T22:32:39.607169+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-01-27T22:32:49.842+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T22:32:39.607169+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:32:49.866+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T22:32:39.607169+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:32:50.691+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:32:51.555+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:32:39.607169+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:32:52.403+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T22:32:39.607169+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:32:52.407+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=to_sqlite, run_id=manual__2024-01-27T22:32:39.607169+00:00, map_index=-1, run_start_date=2024-01-27 22:32:51.628336+00:00, run_end_date=2024-01-27 22:32:51.918575+00:00, run_duration=0.290239, state=success, executor_state=success, try_number=1, max_tries=0, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-01-27 22:32:49.841146+00:00, queued_by_job_id=15, pid=48956[0m
[[34m2024-01-27T22:32:52.951+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun ETL_1 @ 2024-01-27 22:32:39.607169+00:00: manual__2024-01-27T22:32:39.607169+00:00, state:running, queued_at: 2024-01-27 22:32:39.614078+00:00. externally triggered: True> successful[0m
[[34m2024-01-27T22:32:52.952+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=ETL_1, execution_date=2024-01-27 22:32:39.607169+00:00, run_id=manual__2024-01-27T22:32:39.607169+00:00, run_start_date=2024-01-27 22:32:40.446204+00:00, run_end_date=2024-01-27 22:32:52.952392+00:00, run_duration=12.506188, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-27 22:32:39.607169+00:00, data_interval_end=2024-01-27 22:32:39.607169+00:00, dag_hash=8fb487296442ad33a0ea4c489351f2db[0m
[[34m2024-01-27T22:37:40.329+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:37:39.447909+00:00 [scheduled]>[0m
[[34m2024-01-27T22:37:40.329+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:37:40.330+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.extract_task manual__2024-01-27T22:37:39.447909+00:00 [scheduled]>[0m
[[34m2024-01-27T22:37:40.331+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:37:39.447909+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-01-27T22:37:40.331+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:37:39.447909+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:37:40.356+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'extract_task', 'manual__2024-01-27T22:37:39.447909+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:37:41.184+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:37:42.902+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.extract_task manual__2024-01-27T22:37:39.447909+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:37:44.915+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='extract_task', run_id='manual__2024-01-27T22:37:39.447909+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:37:44.918+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=extract_task, run_id=manual__2024-01-27T22:37:39.447909+00:00, map_index=-1, run_start_date=2024-01-27 22:37:43.138109+00:00, run_end_date=2024-01-27 22:37:44.508672+00:00, run_duration=1.370563, state=success, executor_state=success, try_number=1, max_tries=0, job_id=52, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-01-27 22:37:40.330490+00:00, queued_by_job_id=15, pid=50875[0m
[[34m2024-01-27T22:37:44.958+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-01-27T22:37:45.032+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:37:39.447909+00:00 [scheduled]>[0m
[[34m2024-01-27T22:37:45.032+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:37:45.032+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.transform_data manual__2024-01-27T22:37:39.447909+00:00 [scheduled]>[0m
[[34m2024-01-27T22:37:45.034+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:37:39.447909+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-01-27T22:37:45.034+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:37:39.447909+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:37:45.060+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'transform_data', 'manual__2024-01-27T22:37:39.447909+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:37:45.828+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:37:46.703+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.transform_data manual__2024-01-27T22:37:39.447909+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:37:47.409+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='transform_data', run_id='manual__2024-01-27T22:37:39.447909+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:37:47.412+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=transform_data, run_id=manual__2024-01-27T22:37:39.447909+00:00, map_index=-1, run_start_date=2024-01-27 22:37:46.772046+00:00, run_end_date=2024-01-27 22:37:47.023686+00:00, run_duration=0.25164, state=success, executor_state=success, try_number=1, max_tries=0, job_id=53, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-01-27 22:37:45.033315+00:00, queued_by_job_id=15, pid=50891[0m
[[34m2024-01-27T22:37:47.477+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:37:39.447909+00:00 [scheduled]>[0m
[[34m2024-01-27T22:37:47.477+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:37:47.477+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:37:39.447909+00:00 [scheduled]>[0m
[[34m2024-01-27T22:37:47.478+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='construct_load_command', run_id='manual__2024-01-27T22:37:39.447909+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-01-27T22:37:47.479+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'construct_load_command', 'manual__2024-01-27T22:37:39.447909+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:37:47.503+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'construct_load_command', 'manual__2024-01-27T22:37:39.447909+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:37:48.332+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:37:49.194+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.construct_load_command manual__2024-01-27T22:37:39.447909+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:37:50.023+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='construct_load_command', run_id='manual__2024-01-27T22:37:39.447909+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:37:50.026+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=construct_load_command, run_id=manual__2024-01-27T22:37:39.447909+00:00, map_index=-1, run_start_date=2024-01-27 22:37:49.349313+00:00, run_end_date=2024-01-27 22:37:49.643430+00:00, run_duration=0.294117, state=success, executor_state=success, try_number=1, max_tries=0, job_id=54, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-01-27 22:37:47.477961+00:00, queued_by_job_id=15, pid=50906[0m
[[34m2024-01-27T22:37:50.124+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:37:39.447909+00:00 [scheduled]>[0m
[[34m2024-01-27T22:37:50.125+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG ETL_1 has 0/16 running and queued tasks[0m
[[34m2024-01-27T22:37:50.125+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:37:39.447909+00:00 [scheduled]>[0m
[[34m2024-01-27T22:37:50.126+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T22:37:39.447909+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-01-27T22:37:50.127+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T22:37:39.447909+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:37:50.153+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_1', 'to_sqlite', 'manual__2024-01-27T22:37:39.447909+00:00', '--local', '--subdir', 'DAGS_FOLDER/ETL_1.py'][0m
[[34m2024-01-27T22:37:50.927+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/ETL_1.py[0m
[[34m2024-01-27T22:37:51.812+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: ETL_1.to_sqlite manual__2024-01-27T22:37:39.447909+00:00 [queued]> on host codespaces-ca3784[0m
[[34m2024-01-27T22:37:52.510+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_1', task_id='to_sqlite', run_id='manual__2024-01-27T22:37:39.447909+00:00', try_number=1, map_index=-1)[0m
[[34m2024-01-27T22:37:52.515+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=ETL_1, task_id=to_sqlite, run_id=manual__2024-01-27T22:37:39.447909+00:00, map_index=-1, run_start_date=2024-01-27 22:37:51.881317+00:00, run_end_date=2024-01-27 22:37:52.178123+00:00, run_duration=0.296806, state=success, executor_state=success, try_number=1, max_tries=0, job_id=55, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-01-27 22:37:50.125792+00:00, queued_by_job_id=15, pid=50921[0m
[[34m2024-01-27T22:37:52.558+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun ETL_1 @ 2024-01-27 22:37:39.447909+00:00: manual__2024-01-27T22:37:39.447909+00:00, state:running, queued_at: 2024-01-27 22:37:39.454805+00:00. externally triggered: True> successful[0m
[[34m2024-01-27T22:37:52.559+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=ETL_1, execution_date=2024-01-27 22:37:39.447909+00:00, run_id=manual__2024-01-27T22:37:39.447909+00:00, run_start_date=2024-01-27 22:37:40.262715+00:00, run_end_date=2024-01-27 22:37:52.559236+00:00, run_duration=12.296521, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-27 22:37:39.447909+00:00, data_interval_end=2024-01-27 22:37:39.447909+00:00, dag_hash=8fb487296442ad33a0ea4c489351f2db[0m
[[34m2024-01-27T22:38:58.617+0000[0m] {[34mscheduler_job_runner.py:[0m247} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2024-01-27T22:38:59.619+0000[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 3119. PIDs of all processes in the group: [3119][0m
[[34m2024-01-27T22:38:59.620+0000[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 3119[0m
[[34m2024-01-27T22:38:59.874+0000[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=3119, status='terminated', exitcode=0, started='20:32:38') (3119) terminated with exit code 0[0m
[[34m2024-01-27T22:38:59.876+0000[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 3119. PIDs of all processes in the group: [][0m
[[34m2024-01-27T22:38:59.876+0000[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 3119[0m
[[34m2024-01-27T22:38:59.877+0000[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 3119 as process group is missing.[0m
[[34m2024-01-27T22:38:59.877+0000[0m] {[34mscheduler_job_runner.py:[0m864} INFO[0m - Exited execute loop[0m
